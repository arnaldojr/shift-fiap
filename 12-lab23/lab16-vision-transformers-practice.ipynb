{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1cb5a79",
   "metadata": {},
   "source": [
    "# Laboratório Prático: Vision Transformers\n",
    "\n",
    "Este notebook demonstra a implementação prática dos Vision Transformers, desde sua construção básica até aplicações avançadas. Vamos explorar a arquitetura, funcionamento e aplicações desses modelos revolucionários de visão computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847bcbc",
   "metadata": {},
   "source": [
    "## 1. Setup e Instalação de Dependências\n",
    "\n",
    "Primeiro, vamos garantir que temos todas as bibliotecas necessárias instaladas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente para instalar as dependências\n",
    "!pip install torch torchvision timm transformers matplotlib einops opencv-python tqdm pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f44c90f",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'einops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, random_split\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, reduce, repeat\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rearrange\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einops'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Verifique se temos GPU disponível\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d924bf2",
   "metadata": {},
   "source": [
    "## 2. Implementação do Vision Transformer (ViT) do Zero\n",
    "\n",
    "Vamos implementar um Vision Transformer passo a passo para entender cada componente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43357e9d",
   "metadata": {},
   "source": [
    "### 2.1 Componentes Básicos do Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba64ef4",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Converte uma imagem em embeddings de patches.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=3, patch_size=16, emb_dim=768, img_size=224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        # Número de patches em cada dimensão\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        # Projeção linear (implementada como convolução)\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            emb_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, channels, height, width]\n",
    "        x = self.proj(x)  # [batch, emb_dim, num_patches^0.5, num_patches^0.5]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')  # [batch, num_patches, emb_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Implementação do bloco Multi-Head Self-Attention\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_dim=768, num_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=emb_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attn_output, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        return x + self.dropout(attn_output)\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Bloco MLP com GELU\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_dim=768, mlp_dim=3072, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.mlp(self.ln(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Bloco Transformer: Attention + MLP\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_dim=768, num_heads=12, mlp_dim=3072, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.attention = AttentionBlock(emb_dim, num_heads, dropout)\n",
    "        self.mlp = MLPBlock(emb_dim, mlp_dim, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.attention(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f288bf3",
   "metadata": {},
   "source": [
    "### 2.2 Arquitetura Completa do Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774ff9d",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Implementação do Vision Transformer (ViT)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size=224, \n",
    "        patch_size=16, \n",
    "        in_channels=3, \n",
    "        num_classes=1000,\n",
    "        emb_dim=768, \n",
    "        depth=12, \n",
    "        num_heads=12, \n",
    "        mlp_dim=3072, \n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding de patches\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            in_channels=in_channels,\n",
    "            patch_size=patch_size,\n",
    "            emb_dim=emb_dim,\n",
    "            img_size=img_size\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # Token de classe (CLS) e embedding posicional\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, emb_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Blocos Transformer\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(emb_dim, num_heads, mlp_dim, dropout) for _ in range(depth)]\n",
    "        )\n",
    "        \n",
    "        # Layer norm final e classificador\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.head = nn.Linear(emb_dim, num_classes)\n",
    "        \n",
    "        # Inicialização dos parâmetros\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, channels, height, width]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Embedding de patches\n",
    "        x = self.patch_embed(x)  # [batch_size, num_patches, emb_dim]\n",
    "        \n",
    "        # Adiciona token de classe\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # [batch_size, 1, emb_dim]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [batch_size, num_patches+1, emb_dim]\n",
    "        \n",
    "        # Adiciona embedding posicional\n",
    "        x = x + self.pos_embed  # [batch_size, num_patches+1, emb_dim]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Passa pelos blocos Transformer\n",
    "        x = self.blocks(x)  # [batch_size, num_patches+1, emb_dim]\n",
    "        x = self.ln(x)  # [batch_size, num_patches+1, emb_dim]\n",
    "        \n",
    "        # Usa o token CLS para classificação\n",
    "        x = x[:, 0]  # [batch_size, emb_dim]\n",
    "        x = self.head(x)  # [batch_size, num_classes]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b3e04",
   "metadata": {},
   "source": [
    "### 2.3 Visualização do Processo de Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(img_path, img_size=224):\n",
    "    \"\"\"Carrega e pré-processa uma imagem para visualização.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    return img, transform(img)\n",
    "\n",
    "def visualize_patches(img_tensor, patch_size=16):\n",
    "    \"\"\"Visualiza o processo de divisão de imagem em patches.\"\"\"\n",
    "    # Convertendo o tensor para numpy para visualização\n",
    "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Obtém dimensões da imagem\n",
    "    h, w, c = img_np.shape\n",
    "    \n",
    "    # Criando grade de patches\n",
    "    fig, axs = plt.subplots(h // patch_size, w // patch_size, figsize=(10, 10))\n",
    "    \n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            # Extrai um patch\n",
    "            patch = img_np[i:i+patch_size, j:j+patch_size, :]\n",
    "            # Mostra o patch na posição correta da grade\n",
    "            axs[i//patch_size, j//patch_size].imshow(patch)\n",
    "            axs[i//patch_size, j//patch_size].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Imagem dividida em patches de {patch_size}x{patch_size}', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.94)\n",
    "    plt.show()\n",
    "\n",
    "# Para demonstrar, baixe uma imagem de exemplo\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Cria diretório de dados se não existir\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# URL da imagem de exemplo (Creative Commons)\n",
    "img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Golden_retriever_standing_in_a_park.jpg/640px-Golden_retriever_standing_in_a_park.jpg\"\n",
    "img_path = data_dir / \"sample_dog.jpg\"\n",
    "\n",
    "# Baixa a imagem se ela não existir localmente\n",
    "if not img_path.exists():\n",
    "    urllib.request.urlretrieve(img_url, img_path)\n",
    "    print(f\"Imagem baixada para {img_path}\")\n",
    "else:\n",
    "    print(f\"Usando imagem existente em {img_path}\")\n",
    "\n",
    "# Visualiza a imagem original e seus patches\n",
    "img, img_tensor = load_and_preprocess_image(img_path)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Imagem Original\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Visualiza os patches\n",
    "visualize_patches(img_tensor, patch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff447202",
   "metadata": {},
   "source": [
    "### 2.4 Inicialização e Inferência com ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb22dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um ViT com configurações reduzidas para fins de demonstração\n",
    "tiny_vit = VisionTransformer(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=1000,\n",
    "    emb_dim=192,        # Reduzido de 768\n",
    "    depth=4,            # Reduzido de 12\n",
    "    num_heads=3,        # Reduzido de 12\n",
    "    mlp_dim=768,        # Reduzido de 3072\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Movendo para o dispositivo disponível\n",
    "tiny_vit = tiny_vit.to(device)\n",
    "\n",
    "# Contando parâmetros\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Parâmetros treináveis do modelo: {count_parameters(tiny_vit):,}\")\n",
    "\n",
    "# Gerando uma entrada aleatória para testar o modelo\n",
    "batch_size = 4\n",
    "x = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "\n",
    "# Executando inferência\n",
    "with torch.no_grad():\n",
    "    output = tiny_vit(x)\n",
    "\n",
    "print(f\"Shape da entrada: {x.shape}\")\n",
    "print(f\"Shape da saída: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb08df",
   "metadata": {},
   "source": [
    "## 3. Usando Modelos Pré-Treinados com HuggingFace e timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9e01e",
   "metadata": {},
   "source": [
    "### 3.1 Carregando um ViT Pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f24a5",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Carregando modelo usando timm\n",
    "print(\"Modelos ViT disponíveis em timm:\")\n",
    "vit_models = [m for m in timm.list_models() if 'vit' in m]\n",
    "for i, m in enumerate(vit_models[:10]):  # Mostrar apenas os primeiros 10\n",
    "    print(f\"  {i+1}. {m}\")\n",
    "print(f\"... e mais {len(vit_models)-10} modelos\")\n",
    "\n",
    "# Carregando um modelo ViT-Base pré-treinado no ImageNet\n",
    "model_timm = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "model_timm.eval().to(device)\n",
    "\n",
    "# Carregando o mesmo modelo usando HuggingFace Transformers\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model_hf = ViTForImageClassification.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e015019",
   "metadata": {},
   "source": [
    "### 3.2 Preparando Imagens para Inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373e316",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_image(img_path, processor=None):\n",
    "    \"\"\"Prepara uma imagem para inferência em ViT.\"\"\"\n",
    "    if processor:\n",
    "        # Usando o processador do HuggingFace\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        return inputs.pixel_values.to(device)\n",
    "    else:\n",
    "        # Preparação padrão para timm\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# Baixa ImageNet labels para interpretação da saída\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Tentar baixar labels do ImageNet se não existirem localmente\n",
    "labels_path = 'data/imagenet_labels.json'\n",
    "if not os.path.exists(labels_path):\n",
    "    os.makedirs(os.path.dirname(labels_path), exist_ok=True)\n",
    "    try:\n",
    "        url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "        urllib.request.urlretrieve(url, labels_path)\n",
    "        print(f\"Labels baixados para {labels_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar labels: {e}\")\n",
    "        # Criar um arquivo vazio de fallback\n",
    "        with open(labels_path, 'w') as f:\n",
    "            json.dump([f\"class_{i}\" for i in range(1000)], f)\n",
    "\n",
    "# Carregando os labels\n",
    "with open(labels_path, 'r') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496127d",
   "metadata": {},
   "source": [
    "### 3.3 Realizando Inferência com Modelos Pré-treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_vit(img_path, model, processor=None, top_k=5):\n",
    "    \"\"\"Faz previsões em uma imagem usando modelo ViT.\"\"\"\n",
    "    # Prepara a imagem\n",
    "    img_tensor = prepare_image(img_path, processor)\n",
    "    \n",
    "    # Exibe a imagem original\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Inferência\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, 'forward') and 'pixel_values' in model.forward.__code__.co_varnames:\n",
    "            # Modelo HuggingFace\n",
    "            outputs = model(pixel_values=img_tensor)\n",
    "            logits = outputs.logits\n",
    "        else:\n",
    "            # Modelo timm\n",
    "            logits = model(img_tensor)\n",
    "    \n",
    "    # Obtém as probabilidades com softmax\n",
    "    probs = F.softmax(logits, dim=1)[0]\n",
    "    \n",
    "    # Pega as top-k classes\n",
    "    top_probs, top_idxs = torch.topk(probs, top_k)\n",
    "    \n",
    "    # Mostra os resultados\n",
    "    for i, (idx, prob) in enumerate(zip(top_idxs.cpu().numpy(), top_probs.cpu().numpy())):\n",
    "        print(f\"{i+1}. {labels[idx]}: {prob:.4f} ({100*prob:.2f}%)\")\n",
    "\n",
    "# Usa a imagem de cachorro baixada anteriormente\n",
    "print(\"\\nPrevisões com modelo timm:\")\n",
    "predict_with_vit(img_path, model_timm)\n",
    "\n",
    "print(\"\\nPrevisões com modelo HuggingFace:\")\n",
    "predict_with_vit(img_path, model_hf, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224a8e6",
   "metadata": {},
   "source": [
    "## 4. Visualizando a Atenção em Vision Transformers\n",
    "\n",
    "Uma das vantagens dos transformers é a capacidade de interpretar como o modelo \"olha\" para a imagem através dos mapas de atenção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b55fd",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from transformers import ViTForImageClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Carregar modelo com outputs de atenção\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "model_attn = ViTForImageClassification.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "model_attn.eval()\n",
    "\n",
    "def get_attention_maps(img_path, model, processor):\n",
    "    \"\"\"Extrai mapas de atenção de um modelo ViT para uma imagem.\"\"\"\n",
    "    # Prepara a imagem\n",
    "    img_tensor = prepare_image(img_path, processor)\n",
    "    \n",
    "    # Inferência com saídas de atenção\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=img_tensor, output_attentions=True)\n",
    "    \n",
    "    # Extrai atenções - shape é (batch, num_heads, seq_len, seq_len)\n",
    "    attentions = outputs.attentions\n",
    "    \n",
    "    return attentions\n",
    "\n",
    "def visualize_attention(img_path, attentions, layer=11, head=0):\n",
    "    \"\"\"Visualiza um mapa de atenção específico sobre uma imagem.\"\"\"\n",
    "    # Carrega a imagem\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = img.resize((224, 224))\n",
    "    img_np = np.array(img)\n",
    "    \n",
    "    # Extrai atenção do token CLS para patches\n",
    "    # O tensor de atenção tem shape (batch, num_heads, seq_len, seq_len)\n",
    "    # O CLS token está na posição 0, então pegamos a primeira linha\n",
    "    attention = attentions[layer][0, head, 0, 1:].reshape(14, 14)\n",
    "    \n",
    "    # Normaliza valores de atenção para [0, 1]\n",
    "    attention_resized = F.interpolate(attention.unsqueeze(0).unsqueeze(0), \n",
    "                             size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    attention_resized = attention_resized.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Visualiza\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    ax1.imshow(img_np)\n",
    "    ax1.set_title(\"Imagem Original\")\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(attention_resized, cmap='viridis')\n",
    "    ax2.set_title(f\"Mapa de Atenção (Layer {layer}, Head {head})\")\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Sobreposição\n",
    "    attention_heatmap = np.uint8(plt.cm.viridis(attention_resized)[:, :, :3] * 255)\n",
    "    overlay = cv2.addWeighted(img_np, 0.6, attention_heatmap, 0.4, 0)\n",
    "    ax3.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "    ax3.set_title(\"Sobreposição\")\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attention\n",
    "\n",
    "# Extrai mapas de atenção para a imagem de cachorro\n",
    "attentions = get_attention_maps(img_path, model_attn, processor)\n",
    "\n",
    "# Visualiza a atenção em diferentes camadas e cabeças\n",
    "for layer in [0, 5, 11]:  # Início, meio e fim do modelo\n",
    "    for head in [0, 5]:  # Diferentes cabeças de atenção\n",
    "        _ = visualize_attention(img_path, attentions, layer=layer, head=head)\n",
    "        print(f\"Camada {layer}, Cabeça {head}: Observe como diferentes cabeças atendem a diferentes aspectos da imagem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe547ca",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning de ViT em um Dataset Personalizado\n",
    "\n",
    "Vamos demonstrar como fazer fine-tuning de um Vision Transformer pré-treinado para uma tarefa específica usando o dataset CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f9667",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função para carregar o CIFAR-10\n",
    "def load_cifar10(batch_size=64):\n",
    "    # Transformações para o CIFAR-10 (redimensionar para 224x224 para o ViT)\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    \n",
    "    # Carrega datasets\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    \n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    \n",
    "    # Nomes das classes\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    # Criar dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, test_loader, class_names\n",
    "\n",
    "# Vamos ver algumas imagens do dataset\n",
    "train_loader, _, class_names = load_cifar10(batch_size=4)\n",
    "examples = iter(train_loader)\n",
    "images, labels = next(examples)\n",
    "\n",
    "# Mostra algumas imagens\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    # Desfaz a normalização para visualização\n",
    "    img = images[i].permute(1, 2, 0).numpy()\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ccbfc",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fine_tune_vit(num_epochs=3, batch_size=32, lr=1e-4):\n",
    "    \"\"\"Fine-tune um ViT no dataset CIFAR-10.\"\"\"\n",
    "    # Carrega dados\n",
    "    train_loader, test_loader, class_names = load_cifar10(batch_size)\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    # Modelo: vamos usar um ViT pequeno pré-treinado\n",
    "    print(\"Carregando modelo pré-treinado...\")\n",
    "    model = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
    "    \n",
    "    # Modifica a última camada para o número correto de classes\n",
    "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer: AdamW com weight decay\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        # Parâmetros da cabeça com LR mais alto\n",
    "        {'params': model.head.parameters(), 'lr': lr * 10},\n",
    "        # Demais parâmetros com LR padrão\n",
    "        {'params': [p for n, p in model.named_parameters() if 'head' not in n]}\n",
    "    ], lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Scheduler de taxa de aprendizagem\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Função de perda\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Métricas de treinamento\n",
    "    best_acc = 0.0\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    \n",
    "    # Loop de treinamento\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        \n",
    "        # Loop de treinamento\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for images, labels in train_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass e otimização\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Estatísticas\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            train_bar.set_postfix({'loss': running_loss/(train_bar.n+1), 'acc': 100.*correct/total})\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Avaliação\n",
    "        model.eval()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "            for images, labels in val_bar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Estatísticas\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                val_bar.set_postfix({'loss': running_loss/(val_bar.n+1), 'acc': 100.*correct/total})\n",
    "        \n",
    "        val_loss = running_loss / len(test_loader)\n",
    "        val_acc = 100. * correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Atualiza scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Salva melhor modelo\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            # Descomente para salvar o modelo\n",
    "            # torch.save(model.state_dict(), 'vit_cifar10_best.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: '\\\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\\\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    # Plotar resultados do treinamento\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Treino')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='Validação')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.title('Evolução da Perda')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs+1), train_accs, 'b-', label='Treino')\n",
    "    plt.plot(range(1, num_epochs+1), val_accs, 'r-', label='Validação')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Acurácia (%)')\n",
    "    plt.title('Evolução da Acurácia')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, best_acc\n",
    "\n",
    "# Descomente para executar o fine-tuning\n",
    "# Para economizar tempo, use apenas 1 época\n",
    "# model_finetuned, best_acc = fine_tune_vit(num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afc4e3",
   "metadata": {},
   "source": [
    "## 6. Utilizando Vision Transformers em Tarefas de Detecção de Objetos\n",
    "\n",
    "Vamos utilizar o DETR (*DEtection TRansformer*), um modelo que combina Transformers com CNNs para detecção de objetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa DETR do HuggingFace\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def detect_objects_with_detr(image_path, threshold=0.9):\n",
    "    \"\"\"Detecta objetos em uma imagem usando o modelo DETR.\"\"\"\n",
    "    # Carrega o modelo e o processador\n",
    "    try:\n",
    "        processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        model.to(device)\n",
    "        \n",
    "        # Carrega a imagem\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Prepara a imagem para o modelo\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Inferência\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Converte saídas para o formato do COCO\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=threshold)[0]\n",
    "        \n",
    "        # Desenha os resultados na imagem\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        # COCO classes\n",
    "        CLASSES = [\n",
    "            'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "            'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "            'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "            'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "            'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "            'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "            'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "            'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "            'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "            'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "            'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "            'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "        ]\n",
    "        \n",
    "        detections = []\n",
    "        \n",
    "        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "            box = [round(i) for i in box.tolist()]\n",
    "            class_name = CLASSES[label]\n",
    "            \n",
    "            # Cor baseada na classe para melhor visualização\n",
    "            color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "            \n",
    "            # Desenha caixa\n",
    "            draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=color, width=3)\n",
    "            \n",
    "            # Texto com classe e score\n",
    "            text = f\"{class_name}: {score.item():.2f}\"\n",
    "            draw.text((box[0], box[1]), text, fill=color)\n",
    "            \n",
    "            detections.append({\n",
    "                \"class\": class_name,\n",
    "                \"score\": score.item(),\n",
    "                \"box\": box\n",
    "            })\n",
    "        \n",
    "        # Mostra a imagem com as detecções\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title('Detecção de Objetos com DETR (Transformer-based)')\n",
    "        plt.show()\n",
    "        \n",
    "        # Lista as detecções\n",
    "        print(f\"Detectados {len(detections)} objetos:\")\n",
    "        for i, det in enumerate(detections):\n",
    "            print(f\"{i+1}. {det['class']} (confiança: {det['score']:.2f})\")\n",
    "        \n",
    "        return image, detections\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar ou executar o modelo DETR: {e}\")\n",
    "        print(\"Para usar esta funcionalidade, instale as dependências necessárias:\")\n",
    "        print(\"pip install transformers timm torch\")\n",
    "        return None, []\n",
    "\n",
    "# Vamos baixar uma imagem com múltiplos objetos\n",
    "street_img_url = \"https://upload.wikimedia.org/wikipedia/commons/b/bd/Broadway_and_Times_Square_by_night.jpg\"\n",
    "street_img_path = data_dir / \"street_scene.jpg\"\n",
    "\n",
    "if not street_img_path.exists():\n",
    "    urllib.request.urlretrieve(street_img_url, street_img_path)\n",
    "    print(f\"Imagem baixada para {street_img_path}\")\n",
    "else:\n",
    "    print(f\"Usando imagem existente em {street_img_path}\")\n",
    "\n",
    "# Descomente para executar a detecção de objetos\n",
    "# image_with_detections, detections = detect_objects_with_detr(street_img_path, threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5593c",
   "metadata": {},
   "source": [
    "## 7. Modelos Auto-Supervisionados: Explorando DINO\n",
    "\n",
    "DINO (Self-Distillation with No Labels) é um método de aprendizado auto-supervisionado para Vision Transformers que produz representações úteis sem necessidade de rótulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829362a2",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_dino_attention(img_path, threshold=None):\n",
    "    \"\"\"Visualiza mapas de atenção do DINO para uma imagem.\"\"\"\n",
    "    try:\n",
    "        # Tenta importar o modelo DINO\n",
    "        import torch.hub\n",
    "        \n",
    "        # Carrega modelo DINO pré-treinado\n",
    "        print(\"Carregando modelo DINO...\")\n",
    "        dino_model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "        dino_model.eval().to(device)\n",
    "        \n",
    "        # Carregar e preprocessar a imagem\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Extrai atenção do último bloco\n",
    "        with torch.no_grad():\n",
    "            # Extrai atenção\n",
    "            outputs = dino_model.get_last_selfattention(img_tensor)\n",
    "            \n",
    "        # Atenção do token CLS para os patches\n",
    "        attn = outputs[:, :, 0, 1:].reshape(1, -1, 14, 14)\n",
    "        attn = torch.mean(attn, dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Aplicar thresholding se especificado\n",
    "        if threshold is not None:\n",
    "            # Normaliza a atenção para [0, 1]\n",
    "            attn_norm = (attn - attn.min()) / (attn.max() - attn.min())\n",
    "            # Aplica threshold\n",
    "            attn_norm = np.where(attn_norm > threshold, 1.0, 0.0)\n",
    "            attn = attn_norm\n",
    "        \n",
    "        # Redimensiona para o tamanho da imagem\n",
    "        img = img.resize((224, 224))\n",
    "        img_np = np.array(img)\n",
    "        attn_resized = cv2.resize(attn, (img_np.shape[1], img_np.shape[0]))\n",
    "        \n",
    "        # Converte a atenção para um mapa de calor\n",
    "        attn_heatmap = cv2.applyColorMap(np.uint8(255 * attn_resized), cv2.COLORMAP_JET)\n",
    "        \n",
    "        # Mistura a imagem original com o mapa de calor\n",
    "        attn_overlay = cv2.addWeighted(cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR), 0.6, attn_heatmap, 0.4, 0)\n",
    "        \n",
    "        # Visualiza\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Imagem Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(attn_resized, cmap='viridis')\n",
    "        plt.title(\"Mapa de Atenção DINO\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(cv2.cvtColor(attn_overlay, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Sobreposição\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return attn\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar ou executar o modelo DINO: {e}\")\n",
    "        print(\"Para usar esta funcionalidade, instale as dependências necessárias:\")\n",
    "        print(\"pip install torch torchvision matplotlib opencv-python\")\n",
    "        return None\n",
    "\n",
    "# Descomente para visualizar a atenção DINO em uma imagem\n",
    "# print(\"Gerando visualização de atenção do DINO na imagem de cachorro...\")\n",
    "# _ = visualize_dino_attention(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48839cb",
   "metadata": {},
   "source": [
    "## 8. Comparação de Performance: ViT vs CNN\n",
    "\n",
    "Vamos comparar a performance de inferência entre um ViT e uma CNN tradicional como a ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4560eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torchvision.models as models\n",
    "\n",
    "def benchmark_inference(model_name, batch_sizes=[1, 4, 16, 32], input_size=(3, 224, 224), runs=50):\n",
    "    \"\"\"Benchmark de inferência para modelos de visão.\"\"\"\n",
    "    results = {'batch_size': [], 'latency_ms': [], 'throughput': []}\n",
    "    \n",
    "    try:\n",
    "        # Carregando o modelo\n",
    "        if model_name == \"vit\":\n",
    "            model = timm.create_model('vit_base_patch16_224', pretrained=False)\n",
    "        elif model_name == \"resnet50\":\n",
    "            model = models.resnet50(pretrained=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo desconhecido: {model_name}\")\n",
    "            \n",
    "        model.eval().to(device)\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # Gera dados de entrada\n",
    "            dummy_input = torch.randn(batch_size, *input_size).to(device)\n",
    "            \n",
    "            # Warm-up\n",
    "            with torch.no_grad():\n",
    "                for _ in range(10):\n",
    "                    _ = model(dummy_input)\n",
    "            \n",
    "            # Mede inferência\n",
    "            latencies = []\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(runs):\n",
    "                    start_time = time.time()\n",
    "                    _ = model(dummy_input)\n",
    "                    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                    latencies.append(time.time() - start_time)\n",
    "            \n",
    "            # Calcula métricas\n",
    "            avg_latency = sum(latencies) / len(latencies) * 1000  # em ms\n",
    "            imgs_per_sec = batch_size / (avg_latency / 1000)\n",
    "            \n",
    "            results['batch_size'].append(batch_size)\n",
    "            results['latency_ms'].append(avg_latency)\n",
    "            results['throughput'].append(imgs_per_sec)\n",
    "            \n",
    "            print(f\"Modelo {model_name}, Batch {batch_size}: {avg_latency:.2f} ms, {imgs_per_sec:.2f} imgs/seg\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante benchmark: {e}\")\n",
    "        return results\n",
    "\n",
    "def plot_benchmark(vit_results, resnet_results):\n",
    "    \"\"\"Plota os resultados do benchmark.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Latência\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(vit_results['batch_size'], vit_results['latency_ms'], 'b-o', label='ViT-Base')\n",
    "    plt.plot(resnet_results['batch_size'], resnet_results['latency_ms'], 'r-o', label='ResNet-50')\n",
    "    plt.xlabel('Tamanho do Batch')\n",
    "    plt.ylabel('Latência (ms)')\n",
    "    plt.title('Comparação de Latência')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Throughput\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(vit_results['batch_size'], vit_results['throughput'], 'b-o', label='ViT-Base')\n",
    "    plt.plot(resnet_results['batch_size'], resnet_results['throughput'], 'r-o', label='ResNet-50')\n",
    "    plt.xlabel('Tamanho do Batch')\n",
    "    plt.ylabel('Throughput (imgs/s)')\n",
    "    plt.title('Comparação de Throughput')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Descomente para executar o benchmark\n",
    "# Se quiser executar benchmarks mais rápidos, reduza o número de runs e batch_sizes\n",
    "# print(\"Executando benchmark para ViT...\")\n",
    "# vit_results = benchmark_inference(\"vit\", batch_sizes=[1, 2, 4], runs=10)\n",
    "# print(\"\\nExecutando benchmark para ResNet-50...\")\n",
    "# resnet_results = benchmark_inference(\"resnet50\", batch_sizes=[1, 2, 4], runs=10)\n",
    "# plot_benchmark(vit_results, resnet_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a4dc3",
   "metadata": {},
   "source": [
    "## 9. Sumário e Conclusões\n",
    "\n",
    "Neste laboratório, exploramos os Vision Transformers (ViT) em profundidade:\n",
    "\n",
    "1. **Implementação do Zero**: Construímos um ViT do zero para entender sua arquitetura\n",
    "2. **Visualização de Patches**: Demonstramos como as imagens são divididas em patches\n",
    "3. **Modelos Pré-treinados**: Utilizamos ViTs da HuggingFace e timm\n",
    "4. **Visualização da Atenção**: Exploramos como os ViTs \"olham\" para as imagens\n",
    "5. **Fine-tuning**: Adaptamos um ViT para o dataset CIFAR-10\n",
    "6. **Aplicações Avançadas**: Detectamos objetos com DETR e exploramos o DINO\n",
    "7. **Performance**: Comparamos ViT e ResNet em termos de latência e throughput\n",
    "\n",
    "Os Vision Transformers representam uma mudança de paradigma na visão computacional, competindo e frequentemente superando as CNNs tradicionais, especialmente em regimes de dados abundantes e para tarefas que requerem compreensão global de imagens.\n",
    "\n",
    "No entanto, eles também apresentam desafios em termos de eficiência computacional e escalabilidade para resoluções maiores. Arquiteturas híbridas e otimizadas continuam surgindo para abordar esses desafios, tornando os transformers cada vez mais práticos para aplicações do mundo real."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
